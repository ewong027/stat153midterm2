---
title: "Midterm Two Project"
author: "Erica Wong (SID:24302634) and Bryana Gutierrez (SID:24504003)"
date: "April 18, 2017"
output: pdf_document
---
```{r, include = FALSE}
library(forecast)
#Loading the Data:
data1 <- read.csv("datasets/q1_train.csv", as.is = TRUE)
data1_df <- data.frame(ts(data1))
data1_df$Date <- 1:nrow(data1)

#Cleaning the Data:
ts1 <- data1$activity
```

In this project we received data and were tasked with coming up with the best predictions for the next 104 data points. Since our data are all time series, we took the time series approach. For the purposes of this report, we focused on the first data set Q1. Our first step was to load and format the data so that we may work with it. Then we checked to make sure that the data didn't have any missing values. Q1 did not contain any missing values so then we proceeded to plot the data. 

```{r, echo=FALSE, fig.cap= 'Plot of Original Data'}
plot(ts1, type = "l")
```

From the plot of the original data, Figure 1, we can see that the variance of the data increases with time. Also, given the slight upward trend, we considered that the data might need to be differenced. To deal with the increase in variance we took the log of the data. Since there are negative values in the data, we added a constant of 1.5 which would later be subtracted when we calculated the predictions. The log transformed data looks as follows:

```{r, echo=FALSE, fig.cap= 'Plot of Data Shifted Up 1.5'}
#Taking the log to get rid of the trend
ts2 <- log(ts1+ 1.5)
plot(ts2, type = 'l')
```

This helped take care of the variance that increases with time. Now it almost looks like the data decreases in variance with time. There is still a slight upward trend in the data, to take care of this we differenced the data. This is how the differenced data looked like. 

```{r}
#Doing Different Transformations to get a better idea of the wanted model
#took difference twice so data looked reasonably stationary 
ts3 <- diff(ts2)
plot(ts3, type = "l")
```

This looked fairly stationary. So we moved on to look at what the ACF and PACF could tell us about the remaining data. 

```{r}
#plotted acf
acf(ts3)
pacf(ts3)
```

We can see that there is some strong AR presence from the large PACF values and also some MA presence from some high values in the ACF plot. The ACF plot would imply that the model may be MA(2) because the lags die off after those points. The PACF plot shows that our model may also have some AR(4) component because after those lags the lags become insignificant. To help give us an initial guess at the ARIMA distribution, we used auto.arima. Since an ARIMA model takes differencing into consideration, we used the data prior to differencing for the auto.arima function. R gave us the following output:

```{r}
#running auto.arima to get a better idea of what the model is
auto.arima(ts2)
```

We used this information to help give us a better idea about what our model is and then moved on to conduct some model selection. 

One method of model selection is Cross Validation. First we wrote a function that will run the cross validation process for us. The output of the function will be the vector of mean-squared errors (MSE) from the different cross validation groupings from the same model. To compare the MSEs from each of the groups, we took the average MSEs from each of the models we tested. 

Before actually conducting cross validation, we first had to come up with some models. We knew that in order for our data to look stationary, we must take 1 difference at one point. Additionally, from the PACF and ACF plot, we can see that there are AR(4) and MA(2) components in our model. So, we started off by thinking we should test an ARIMA(4,1,2) model. This model also happen to match what auto.arima() gave us. However, we also noticed from the original data (Figure 1) that there is some seasonality that exists. Since, this is data that is collected every week over years, we believed that the seasonality may be lag 52 because there are 52 weeks in a year. So, that is how we created our original model. We then conducted cross validation with models that were similar to what our original prediction was. 

After coming up with various models and running cross validation, the best model in our case was from model 1 and the second best was model 6. In table 1, we can see a comparison of the MSEs.

```{r, results = "asis", echo = FALSE, message = FALSE, warning = FALSE, fig.align = "center", cache=TRUE}
library(xtable)
CV <- function(test_order, test_seasonality, test_period){
  mse <- NULL
  leng <- length(ts2)
  for (i in 4:1){
    train <- ts2[1:(leng - i*105)]
    test <- ts2[(leng - i*105 + 1):(leng - (i-1)*105)]
    mod_train <- arima(ts2, order = test_order, seasonal = list(order = test_seasonality, 
                                                                period= test_period))
    forcast <- predict(mod_train, n.ahead = 105)
    mse[i] <- mean((exp(forcast$pred) - exp(test))^2)
  }
  return(mse)
}

mse1 <- CV(c(4,1,2),c(1, 0, 1),52)
mse2 <- CV(c(4,2,2),c(1, 0, 1),52)
mse3 <- CV(c(4,1,2),c(0, 0, 1),52)
mse4 <- CV(c(5,1,2),c(1, 0, 1),52)
mse5 <- CV(c(4,1,3),c(1, 0, 1),52)
mse6 <- CV(c(4,1,1),c(1, 0, 1),52)

table <- matrix(data = c(sum(mse1)/4,
                    sum(mse2)/4,
                    sum(mse3)/4,
                    sum(mse4)/4,
                    sum(mse5)/4,
                    sum(mse6)/4), ncol = 1)

colnames(table) <- 'Average MSE'
rownames(table) <- c('Model 1', 'Model 2', 'Model 3', 
                           'Model 4', 'Model 5', 'Model 6')

print(xtable(table, caption = 'MSE Table', digits = 4),
      type = 'latex', comment = FALSE)
```

After conducting cross validation, we still wanted to which model other model selection methods would tell us to pick. So, we conducted AIC and BIC. We knew that for AIC and BIC model selection, we wanted to pick the model with the lowest value. Tables 2 and 3 give us the values of AIC and BIC respectively. The best models for both are model 6, followed by model 4, and then model 1. However, when generating model 4, we did get some errors, so we believed that it would be best to eliminate that model from being considered. 

```{r, results = "asis", echo = FALSE, message = FALSE, warning = FALSE, fig.align = "center", cache=TRUE}
mod_test1 <- arima(ts2, order = c(4,1,2), seasonal = list(order = c(1, 0, 1), period = 52))
mod_test2 <- arima(ts2, order = c(4,2,2), seasonal = list(order = c(1, 0, 1), period = 52))
mod_test3 <- arima(ts2, order = c(4,1,2), seasonal = list(order = c(0, 0, 1), period = 52))
mod_test4 <- arima(ts2, order = c(5,1,2), seasonal = list(order = c(1, 0, 1), period = 52))
mod_test5 <- arima(ts2, order = c(4,1,3), seasonal = list(order = c(1, 0, 1), period = 52))
mod_test6 <- arima(ts2, order = c(4,1,1), seasonal = list(order = c(1, 0, 1), period = 52))

AIC_table <- matrix(data = c(AIC(mod_test1),
                         AIC(mod_test2),
                         AIC(mod_test3),
                         AIC(mod_test4),
                         AIC(mod_test5),
                         AIC(mod_test6)), ncol = 1)

colnames(AIC_table) <- 'AIC Value'
rownames(AIC_table) <- c('Model 1', 'Model 2', 'Model 3', 
                           'Model 4', 'Model 5', 'Model 6')

print(xtable(AIC_table, caption = 'AIC Table', digits = 4),
      type = 'latex', comment = FALSE)
```

```{r, results = "asis", echo = FALSE, message = FALSE, warning = FALSE, fig.align = "center", cache=TRUE}
BIC_table <- matrix(data = c(BIC(mod_test1),
                         BIC(mod_test2),
                         BIC(mod_test3),
                         BIC(mod_test4),
                         BIC(mod_test5),
                         BIC(mod_test6)), ncol = 1)

colnames(BIC_table) <- 'BIC Value'
rownames(BIC_table) <- c('Model 1', 'Model 2', 'Model 3', 
                           'Model 4', 'Model 5', 'Model 6')

print(xtable(BIC_table, caption = 'BIC Table', digits = 4),
      type = 'latex', comment = FALSE)
```

From all three of our model selection processes, we get model 6 and model 1 as the best. So to help make our final decision, we decided to graph both of them to see which one we liked better or believed fit in our model better.
```{r, echo=FALSE, fig.cap= 'Model 1 Plot', message = FALSE, fig.align = "center"}
preds_1 <- predict(mod_test1, n.ahead = 104)
preds2_1 <- exp(preds_1$pred) - 1.5
plot(c(ts1, preds2_1), type = "l")
```

```{r, echo=FALSE, fig.cap= 'Model 6 Plot', message = FALSE, fig.align = "center"}
preds <- predict(mod_test6, n.ahead = 104)
preds2 <- exp(preds$pred) - 1.5
plot(c(ts1, preds2), type = "l")
```

The plots look the same to us, so we just went with model 6 because overall we believed that it performed better in model selection, ranking consistently in the top two best models. So, the final model that we went with was arima(ts2, order = c(4,1,1), seasonal = list(order = c(1, 0, 1), period = 52)). 